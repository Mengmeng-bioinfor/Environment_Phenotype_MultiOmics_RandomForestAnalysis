{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f99b296-7d0c-4866-89ed-e9eeb17b043b",
   "metadata": {},
   "source": [
    "# 🔍 Prediction using Random Forest\n",
    "\n",
    "This notebook performs supervised regression using a Random Forest model to predict **** from environmental and categorical variables.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Objectives\n",
    "\n",
    "- Predict SOC based on environmental predictors and categorical treatment factors.\n",
    "- Rank feature importance.\n",
    "- Visualize and interpret model performance, residuals, and SHAP values.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Workflow\n",
    "\n",
    "1. **Data Preparation**  \n",
    "   - Load and clean CSV data.  \n",
    "   - One-hot encode categorical columns: `Year`, `Species`, `Tre`, `Sub`.\n",
    "\n",
    "2. **Model Training**  \n",
    "   - Random Forest Regression (100 trees, with OOB scoring).  \n",
    "   - Train/test split (70/30).\n",
    "\n",
    "3. **Model Evaluation**  \n",
    "   - Metrics: MSE, RMSE, MAE, R².  \n",
    "   - Feature importance analysis.  \n",
    "   - Residual and complexity analysis.  \n",
    "   - SHAP and Partial Dependence Plots.\n",
    "\n",
    "4. **Feature Selection**  \n",
    "   - Select features with importance > 0.01.  \n",
    "   - Re-train and cross-validate with selected features.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Outputs\n",
    "\n",
    "- `feature_importance04.csv`: Importance scores of predictors.\n",
    "- `feature_importance_plot04.png`: Horizontal bar chart.\n",
    "- `residual_plot04.png`: Actual vs. predicted plot.\n",
    "- `model_complexity_plot04.png`: MSE vs. number of trees.\n",
    "- `pdp_plot04.png`: Partial dependence plots.\n",
    "- SHAP summary plot for feature impact visualization.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Notes\n",
    "\n",
    "- SHAP is used for model interpretability.\n",
    "- Model complexity curve helps avoid overfitting.\n",
    "- Suitable for environmental modeling and field experiment datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Requirements\n",
    "\n",
    "- `pandas`, `scikit-learn`, `matplotlib`, `shap`, `numpy`\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 Example Directory Structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b71f6-ee34-4128-9447-3c2d20840a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r'..SoC.csv'  # Replace with your own path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove whitespace from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Ensure the target column 'SOC' exists\n",
    "if 'SOC' not in df.columns:\n",
    "    raise KeyError(\"SOC column not found in the dataset.\")\n",
    "\n",
    "# One-hot encode categorical variables as interaction terms\n",
    "categorical_cols = ['Year', 'Species', 'Tre', 'Sub']\n",
    "X_categorical = pd.get_dummies(df[categorical_cols], drop_first=True)\n",
    "\n",
    "# Separate numeric features and response variable\n",
    "X_numeric = df.drop(columns=['SOC', 'Year', 'Species', 'Tre', 'Sub'])\n",
    "X = pd.concat([X_numeric, X_categorical], axis=1)\n",
    "Y = df['SOC']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train a Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, oob_score=True)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate model performance\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'R-squared (R²): {r2}')\n",
    "print(f'OOB Score (R²): {rf_model.oob_score_}')\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_importances = rf_model.feature_importances_\n",
    "importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Filter features with importance above threshold\n",
    "threshold = 0.01\n",
    "selected_features = importance_df[importance_df['Importance'] > threshold]['Feature'].tolist()\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "print(f'Selected Features: {selected_features}')\n",
    "\n",
    "# Perform K-fold cross-validation on selected features\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(rf_model, X_selected, Y, cv=kf, scoring='r2')\n",
    "print(f'Cross-validated R²: {cv_scores.mean()} ± {cv_scores.std()}')\n",
    "\n",
    "# SHAP analysis\n",
    "X_train_selected, X_test_selected, y_train, y_test = train_test_split(X_selected, Y, test_size=0.3, random_state=42)\n",
    "rf_model.fit(X_train_selected, y_train)\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(X_test_selected)\n",
    "shap.summary_plot(shap_values, X_test_selected)\n",
    "\n",
    "# Save feature importance to CSV\n",
    "importance_csv_path = r'..feature_importance04.csv'\n",
    "importance_df.to_csv(importance_csv_path, index=False)\n",
    "print(f'Feature importance saved to {importance_csv_path}')\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title(f'Feature Importance from Random Forest (R² = {round(r2, 2)})')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Annotate bar values\n",
    "for index, value in enumerate(importance_df['Importance']):\n",
    "    plt.text(value, index, f'{value:.4f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'..feature_importance_plot04.png')\n",
    "plt.show()\n",
    "\n",
    "# Residual analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--', color='red')\n",
    "plt.xlabel('Actual SOC')\n",
    "plt.ylabel('Predicted SOC')\n",
    "plt.title('Actual vs Predicted SOC')\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'..residual_plot04.png')\n",
    "plt.show()\n",
    "\n",
    "# Model complexity analysis: test error vs. number of trees\n",
    "errors = []\n",
    "for n in range(10, 200, 10):\n",
    "    rf = RandomForestRegressor(n_estimators=n, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_test = rf.predict(X_test)\n",
    "    errors.append(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(10, 200, 10), errors, marker='o')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('Model Complexity Analysis')\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'..model_complexity_plot04.png')\n",
    "plt.show()\n",
    "\n",
    "# Partial Dependence Plot (PDP)\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    rf_model, X_train_selected, features=X_selected.columns, feature_names=X_selected.columns, ax=ax\n",
    ")\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'..pdp_plot04.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
